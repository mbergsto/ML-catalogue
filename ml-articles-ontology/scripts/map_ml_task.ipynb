{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979dd845",
   "metadata": {},
   "source": [
    "# Mapping ML articles to ML tasks\n",
    "\n",
    "The ML tasks are defined in the ML-Ontology (ml-ontology/ML_Ontology.ttl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add85551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f851f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1+cu128\n",
      "cuda available: True\n",
      "torch cuda: 12.8\n",
      "device count: 1\n",
      "gpu name: Tesla P100-PCIE-16GB\n",
      "capability: (6, 0)\n"
     ]
    }
   ],
   "source": [
    "# Cell A: Rask diagnose av GPU/PyTorch\n",
    "import torch\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"torch cuda:\", torch.version.cuda)\n",
    "print(\"device count:\", torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"capability:\", torch.cuda.get_device_capability(0)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22680146",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_tasks_path = \"../data/ml_tasks.csv\"\n",
    "articles_path = \"../data/ml_articles_dataset.csv\"\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "TOP_K = 5\n",
    "THRESHOLD = 0.35\n",
    "BATCH_SIZE = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f624dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.3390/asi6050076</td>\n",
       "      <td>Measuring Carbon in Cities and Their Buildings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1016/j.resconrec.2023.107073</td>\n",
       "      <td>Predictive modeling for the quantity of recycl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.30638/eemj.2023.018</td>\n",
       "      <td>END-OF-LIFE VEHICLES ASSESSMENT OF THE AUTOMOB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1115/DETC2023-114718</td>\n",
       "      <td>PREDICTING THE QUANTITY OF RECYCLED END-OF-LIF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1007/978-3-031-69626-8_78</td>\n",
       "      <td>Machine Learning Integration in LCA: Addressin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               doi  \\\n",
       "0               10.3390/asi6050076   \n",
       "1  10.1016/j.resconrec.2023.107073   \n",
       "2           10.30638/eemj.2023.018   \n",
       "3          10.1115/DETC2023-114718   \n",
       "4     10.1007/978-3-031-69626-8_78   \n",
       "\n",
       "                                               title  \n",
       "0  Measuring Carbon in Cities and Their Buildings...  \n",
       "1  Predictive modeling for the quantity of recycl...  \n",
       "2  END-OF-LIFE VEHICLES ASSESSMENT OF THE AUTOMOB...  \n",
       "3  PREDICTING THE QUANTITY OF RECYCLED END-OF-LIF...  \n",
       "4  Machine Learning Integration in LCA: Addressin...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "tasks = pd.read_csv(ml_tasks_path)\n",
    "articles = pd.read_csv(articles_path)\n",
    "\n",
    "# Task profile : label + description\n",
    "tasks[\"task_text\"] = (tasks[\"taskLabel\"].fillna(\"\") + \". \" + tasks[\"description\"].fillna(\"\")).str.strip()\n",
    "\n",
    "# Document text : abstract + title\n",
    "articles[\"doc_text\"] = (articles[\"title\"].fillna(\"\") + \". \" + articles[\"clean_abs\"].fillna(\"\")).str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07b8b223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>taskLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://h-da.de/ml-ontology/action_recognition</td>\n",
       "      <td>action recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://h-da.de/ml-ontology/anomaly_detection</td>\n",
       "      <td>anomaly detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://h-da.de/ml-ontology/association_rule_le...</td>\n",
       "      <td>association rule learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://h-da.de/ml-ontology/audio_classification</td>\n",
       "      <td>audio classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://h-da.de/ml-ontology/audio_regression</td>\n",
       "      <td>audio regression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                task  \\\n",
       "0      http://h-da.de/ml-ontology/action_recognition   \n",
       "1       http://h-da.de/ml-ontology/anomaly_detection   \n",
       "2  http://h-da.de/ml-ontology/association_rule_le...   \n",
       "3    http://h-da.de/ml-ontology/audio_classification   \n",
       "4        http://h-da.de/ml-ontology/audio_regression   \n",
       "\n",
       "                   taskLabel  \n",
       "0         action recognition  \n",
       "1          anomaly detection  \n",
       "2  association rule learning  \n",
       "3       audio classification  \n",
       "4           audio regression  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks[[\"task\", \"taskLabel\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25569c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>doc_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.3390/asi6050076</td>\n",
       "      <td>Measuring Carbon in Cities and Their Buildings...</td>\n",
       "      <td>Measuring Carbon in Cities and Their Buildings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1016/j.resconrec.2023.107073</td>\n",
       "      <td>Predictive modeling for the quantity of recycl...</td>\n",
       "      <td>Predictive modeling for the quantity of recycl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.30638/eemj.2023.018</td>\n",
       "      <td>END-OF-LIFE VEHICLES ASSESSMENT OF THE AUTOMOB...</td>\n",
       "      <td>END-OF-LIFE VEHICLES ASSESSMENT OF THE AUTOMOB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1115/DETC2023-114718</td>\n",
       "      <td>PREDICTING THE QUANTITY OF RECYCLED END-OF-LIF...</td>\n",
       "      <td>PREDICTING THE QUANTITY OF RECYCLED END-OF-LIF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1007/978-3-031-69626-8_78</td>\n",
       "      <td>Machine Learning Integration in LCA: Addressin...</td>\n",
       "      <td>Machine Learning Integration in LCA: Addressin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               doi  \\\n",
       "0               10.3390/asi6050076   \n",
       "1  10.1016/j.resconrec.2023.107073   \n",
       "2           10.30638/eemj.2023.018   \n",
       "3          10.1115/DETC2023-114718   \n",
       "4     10.1007/978-3-031-69626-8_78   \n",
       "\n",
       "                                               title  \\\n",
       "0  Measuring Carbon in Cities and Their Buildings...   \n",
       "1  Predictive modeling for the quantity of recycl...   \n",
       "2  END-OF-LIFE VEHICLES ASSESSMENT OF THE AUTOMOB...   \n",
       "3  PREDICTING THE QUANTITY OF RECYCLED END-OF-LIF...   \n",
       "4  Machine Learning Integration in LCA: Addressin...   \n",
       "\n",
       "                                            doc_text  \n",
       "0  Measuring Carbon in Cities and Their Buildings...  \n",
       "1  Predictive modeling for the quantity of recycl...  \n",
       "2  END-OF-LIFE VEHICLES ASSESSMENT OF THE AUTOMOB...  \n",
       "3  PREDICTING THE QUANTITY OF RECYCLED END-OF-LIF...  \n",
       "4  Machine Learning Integration in LCA: Addressin...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[[\"doi\", \"title\", \"doc_text\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c47f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length : 384\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Tokenizer + max length\n",
    "tokenizer = model.tokenizer\n",
    "max_len = model.get_max_seq_length()\n",
    "\n",
    "print(\"max length :\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfdcf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding + normalization\n",
    "def get_embedding(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    return model.encode(\n",
    "        text,\n",
    "        show_progress_bar=False,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,  # dot-cosine\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1eede4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Chunking-embedding for lange tekster (unngår truncation)\n",
    "def embed_with_chunking(text, max_len=max_len):\n",
    "    # Håndter NaN/ikke-streng\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "\n",
    "    # Tokeniser til word pieces\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Kort nok -> embed direkte\n",
    "    if len(tokens) <= max_len:\n",
    "        return get_embedding(text)\n",
    "\n",
    "    # Del opp i biter som passer maks-lengden\n",
    "    chunks = [\n",
    "        tokenizer.convert_tokens_to_string(tokens[i:i + max_len])\n",
    "        for i in range(0, len(tokens), max_len)\n",
    "    ]\n",
    "\n",
    "    # Embed hver chunk\n",
    "    emb_chunks = [get_embedding(chunk) for chunk in chunks]\n",
    "\n",
    "    # Gjennomsnitt (beholder dimensjon)\n",
    "    return np.mean(emb_chunks, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3096de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Batchet chunking-embedding (raskere enn 1 og 1)\n",
    "def embed_texts_chunked(texts, batch_size=BATCH_SIZE):\n",
    "    embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        for t in batch:\n",
    "            embs.append(embed_with_chunking(t))\n",
    "    return np.vstack(embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4057205f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc2b2ba47704ce599f35c59517d11b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 7: Lag embeddings for tasks og artikler\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m task_emb = \u001b[43membed_texts_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtask_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m doc_emb  = embed_texts_chunked(articles[\u001b[33m\"\u001b[39m\u001b[33mdoc_text\u001b[39m\u001b[33m\"\u001b[39m].tolist())\n\u001b[32m      5\u001b[39m task_emb.shape, doc_emb.shape\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36membed_texts_chunked\u001b[39m\u001b[34m(texts, batch_size)\u001b[39m\n\u001b[32m      5\u001b[39m     batch = texts[i:i + batch_size]\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         embs.append(\u001b[43membed_with_chunking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.vstack(embs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36membed_with_chunking\u001b[39m\u001b[34m(text, max_len)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Kort nok -> embed direkte\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens) <= max_len:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Del opp i biter som passer maks-lengden\u001b[39;00m\n\u001b[32m     15\u001b[39m chunks = [\n\u001b[32m     16\u001b[39m     tokenizer.convert_tokens_to_string(tokens[i:i + max_len])\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokens), max_len)\n\u001b[32m     18\u001b[39m ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mget_embedding\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m      4\u001b[39m     text = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# dot-cosine\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/ml-catalogue/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/ml-catalogue/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:1094\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1091\u001b[39m features.update(extra_features)\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1096\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/ml-catalogue/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:1175\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m   1169\u001b[39m             module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m   1170\u001b[39m         module_kwargs = {\n\u001b[32m   1171\u001b[39m             key: value\n\u001b[32m   1172\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n\u001b[32m   1173\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33mforward_kwargs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module.forward_kwargs)\n\u001b[32m   1174\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/ml-catalogue/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/ml-catalogue/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/ml-catalogue/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:261\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03mForward pass through the transformer model.\u001b[39;00m\n\u001b[32m    240\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    257\u001b[39m \u001b[33;03m        - 'all_layer_embeddings': If the model outputs hidden states, contains embeddings from all layers\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    259\u001b[39m trans_features = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_forward_params}\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    263\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/ml-catalogue/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/ml-catalogue/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/ml-catalogue/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py:482\u001b[39m, in \u001b[36mMPNetModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    481\u001b[39m     attention_mask = torch.ones(input_shape, device=device)\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m extended_attention_mask: torch.Tensor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_extended_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m    485\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/ml-catalogue/lib/python3.11/site-packages/transformers/modeling_utils.py:1648\u001b[39m, in \u001b[36mModuleUtilsMixin.get_extended_attention_mask\u001b[39m\u001b[34m(self, attention_mask, input_shape, device, dtype)\u001b[39m\n\u001b[32m   1639\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1640\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWrong shape for input_ids (shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) or attention_mask (shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattention_mask.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1641\u001b[39m     )\n\u001b[32m   1643\u001b[39m \u001b[38;5;66;03m# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\u001b[39;00m\n\u001b[32m   1644\u001b[39m \u001b[38;5;66;03m# masked positions, this operation will create a tensor which is 0.0 for\u001b[39;00m\n\u001b[32m   1645\u001b[39m \u001b[38;5;66;03m# positions we want to attend and the dtype's smallest value for masked positions.\u001b[39;00m\n\u001b[32m   1646\u001b[39m \u001b[38;5;66;03m# Since we are adding it to the raw scores before the softmax, this is\u001b[39;00m\n\u001b[32m   1647\u001b[39m \u001b[38;5;66;03m# effectively the same as removing these entirely.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1648\u001b[39m extended_attention_mask = \u001b[43mextended_attention_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# fp16 compatibility\u001b[39;00m\n\u001b[32m   1649\u001b[39m extended_attention_mask = (\u001b[32m1.0\u001b[39m - extended_attention_mask) * torch.finfo(dtype).min\n\u001b[32m   1650\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m extended_attention_mask\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Lag embeddings for tasks og artikler\n",
    "task_emb = embed_texts_chunked(tasks[\"task_text\"].tolist())\n",
    "doc_emb  = embed_texts_chunked(articles[\"doc_text\"].tolist())\n",
    "\n",
    "task_emb.shape, doc_emb.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f1210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-catalogue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
