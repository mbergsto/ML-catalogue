{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271dba2b",
   "metadata": {},
   "source": [
    "- Sentence Transformer for embedding abstracts\n",
    "- Reducing embedding dimensionality with UMAP\n",
    "- Apply HDBSCAN for density-based clustering without pre-defining number of clusters\n",
    "\n",
    "Results:\n",
    "- Large proportion of points classified as noise\n",
    "- Many clusters are small and fragmented\n",
    "- No clear high-level structure emerges across the full dataset\n",
    "    - Indicates that HDBSCAN struggles to identify stable global topics\n",
    "    - Suggests trying K-Means or other clustering approaches on the UMAP projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5177e318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/mbergst/.venvs/ml-catalogue/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f3c200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All directories verified/created.\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "\n",
    "data_path = Path(\"../../data/short-raw-refs-abs\")\n",
    "processed_abstracts_path = Path(\"../../data/processed/abstracts\")\n",
    "save_path = processed_abstracts_path / \"umap_hdbscan\"\n",
    "\n",
    "\n",
    "# Ensure directories exist\n",
    "for p in [data_path, processed_abstracts_path, save_path]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"All directories verified/created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0873186b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 52290 abstracts from 24 queries.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>eid</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>clean_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ml_anomaly_detection_production</td>\n",
       "      <td>2-s2.0-105018574505</td>\n",
       "      <td>10.1016/j.measurement.2025.119261</td>\n",
       "      <td>Distillation anomaly and fault detection based...</td>\n",
       "      <td>© 2025 The Author(s)The detection of anomalies...</td>\n",
       "      <td>Indeed, highly efficient systems do not always...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ml_anomaly_detection_production</td>\n",
       "      <td>2-s2.0-105019192533</td>\n",
       "      <td>10.1007/978-3-032-06118-8_30</td>\n",
       "      <td>From Lab to Factory: Pitfalls and Guidelines f...</td>\n",
       "      <td>© The Author(s), under exclusive license to Sp...</td>\n",
       "      <td>The detection and localization of quality-rela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ml_anomaly_detection_production</td>\n",
       "      <td>2-s2.0-105016669957</td>\n",
       "      <td>10.1007/978-3-032-04200-2_5</td>\n",
       "      <td>Intelligent Defect Detection for Manufacturing...</td>\n",
       "      <td>© The Author(s), under exclusive license to Sp...</td>\n",
       "      <td>In modern Industry, I4.0, artificial intellige...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ml_anomaly_detection_production</td>\n",
       "      <td>2-s2.0-85218693791</td>\n",
       "      <td>10.1038/s41598-025-90810-w</td>\n",
       "      <td>Hybrid machine learning framework for predicti...</td>\n",
       "      <td>© The Author(s) 2025.The critical necessity fo...</td>\n",
       "      <td>The critical necessity for sophisticated predi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ml_anomaly_detection_production</td>\n",
       "      <td>2-s2.0-105018301117</td>\n",
       "      <td>10.1016/j.comnet.2025.111753</td>\n",
       "      <td>BGP anomaly detection using the raw internet t...</td>\n",
       "      <td>© 2025 The AuthorsThe Border Gateway Protocol ...</td>\n",
       "      <td>Hence, detecting any anomaly concerning BGP an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          query_id                  eid  \\\n",
       "0  ml_anomaly_detection_production  2-s2.0-105018574505   \n",
       "1  ml_anomaly_detection_production  2-s2.0-105019192533   \n",
       "2  ml_anomaly_detection_production  2-s2.0-105016669957   \n",
       "3  ml_anomaly_detection_production   2-s2.0-85218693791   \n",
       "4  ml_anomaly_detection_production  2-s2.0-105018301117   \n",
       "\n",
       "                                 doi  \\\n",
       "0  10.1016/j.measurement.2025.119261   \n",
       "1       10.1007/978-3-032-06118-8_30   \n",
       "2        10.1007/978-3-032-04200-2_5   \n",
       "3         10.1038/s41598-025-90810-w   \n",
       "4       10.1016/j.comnet.2025.111753   \n",
       "\n",
       "                                               title  \\\n",
       "0  Distillation anomaly and fault detection based...   \n",
       "1  From Lab to Factory: Pitfalls and Guidelines f...   \n",
       "2  Intelligent Defect Detection for Manufacturing...   \n",
       "3  Hybrid machine learning framework for predicti...   \n",
       "4  BGP anomaly detection using the raw internet t...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  © 2025 The Author(s)The detection of anomalies...   \n",
       "1  © The Author(s), under exclusive license to Sp...   \n",
       "2  © The Author(s), under exclusive license to Sp...   \n",
       "3  © The Author(s) 2025.The critical necessity fo...   \n",
       "4  © 2025 The AuthorsThe Border Gateway Protocol ...   \n",
       "\n",
       "                                           clean_abs  \n",
       "0  Indeed, highly efficient systems do not always...  \n",
       "1  The detection and localization of quality-rela...  \n",
       "2  In modern Industry, I4.0, artificial intellige...  \n",
       "3  The critical necessity for sophisticated predi...  \n",
       "4  Hence, detecting any anomaly concerning BGP an...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load abstracts dataset\n",
    "\n",
    "abstracts_path = processed_abstracts_path / \"abstracts.csv\"\n",
    "df = pd.read_csv(abstracts_path)\n",
    "\n",
    "print(f\"Loaded {len(df)} abstracts from {df['query_id'].nunique()} queries.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afcf6920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 52290\n",
      "After removing duplicates: 33130\n",
      "Remaining duplicate DOIs: 0\n"
     ]
    }
   ],
   "source": [
    "# Count how many rows each query_id has\n",
    "query_counts = df[\"query_id\"].value_counts().to_dict()\n",
    "\n",
    "# Create a copy and map the counts to each row\n",
    "df = df.copy()\n",
    "df[\"query_size\"] = df[\"query_id\"].map(query_counts)\n",
    "\n",
    "# Sort so that query groups with fewer rows are prioritized\n",
    "df_sorted = df.sort_values(by=\"query_size\", ascending=True)\n",
    "\n",
    "# Remove duplicate DOIs, keeping the one in the smallest query group\n",
    "df_dedup = df_sorted.drop_duplicates(subset=\"doi\", keep=\"first\").drop(columns=[\"query_size\"])\n",
    "\n",
    "# Print results\n",
    "print(\"Original dataset size:\", len(df))\n",
    "print(\"After removing duplicates:\", len(df_dedup))\n",
    "print(\"Remaining duplicate DOIs:\", df_dedup[\"doi\"].duplicated().sum())\n",
    "\n",
    "df = df_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee2c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ad139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed all abstracts in batches\n",
    "\n",
    "texts = df[\"clean_abs\"].astype(str).tolist()\n",
    "batch_size = 256\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for start in tqdm(range(0, len(texts), batch_size), desc=\"Embedding abstracts\"):\n",
    "    batch = texts[start:start+batch_size]\n",
    "    batch_emb = model.encode(\n",
    "        batch,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "        batch_size=len(batch)\n",
    "    )\n",
    "    embeddings.append(batch_emb)\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "print(\"Embedding matrix shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99a9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to .npy file\n",
    "emb_path = save_path / \"abstract_embeddings.npy\"\n",
    "np.save(emb_path, embeddings)\n",
    "print(\"Saved embeddings to:\", emb_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527dcdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings back\n",
    "embeddings = np.load(save_path / \"abstract_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0590274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce embedding dimensionality with UMAP\n",
    "\n",
    "umap_model = umap.UMAP(\n",
    "    n_neighbors=30,\n",
    "    min_dist=0.1,\n",
    "    n_components=2,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "umap_coords = umap_model.fit_transform(embeddings)\n",
    "df[\"umap_x\"] = umap_coords[:,0]\n",
    "df[\"umap_y\"] = umap_coords[:,1]\n",
    "\n",
    "print(\"UMAP projection complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a77377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster UMAP-projected data using HDBSCAN\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=200,\n",
    "    min_samples=5,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='leaf'\n",
    ")\n",
    "\n",
    "cluster_labels = clusterer.fit_predict(umap_coords)\n",
    "df[\"cluster_id\"] = cluster_labels\n",
    "\n",
    "print(\"Clusters found:\", df[\"cluster_id\"].nunique(), \"(includes -1 noise)\")\n",
    "df[\"cluster_id\"].value_counts().sort_index().head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a45f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save extended dataset with phase information\n",
    "\n",
    "out_path = save_path / \"abstracts_with_clusters.csv\"\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"Saved dataset with clusters to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a658946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of clusters in UMAP space\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "scatter = plt.scatter(\n",
    "    df[\"umap_x\"],\n",
    "    df[\"umap_y\"],\n",
    "    c=df[\"cluster_id\"],\n",
    "    cmap=\"tab20\",\n",
    "    s=8,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title(\"HDBSCAN Clusters on UMAP Projection\")\n",
    "plt.xlabel(\"UMAP-1\")\n",
    "plt.ylabel(\"UMAP-2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f5bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top queries per cluster (themes)\n",
    "cluster_summary = (\n",
    "    df.groupby([\"cluster_id\", \"query_id\"])\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    "      .sort_values([\"cluster_id\", \"count\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "cluster_summary.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: cluster sizes and % of noise\n",
    "cluster_counts = df[\"cluster_id\"].value_counts()\n",
    "noise_pct = 100 * cluster_counts.get(-1, 0) / len(df)\n",
    "\n",
    "print(\"Cluster count summary:\")\n",
    "print(cluster_counts.head(10))\n",
    "print(f\"\\nNoise articles: {noise_pct:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show one representative abstract per cluster (closest to centroid)\n",
    "\n",
    "cluster_reps = []\n",
    "\n",
    "for cid in sorted(df[\"cluster_id\"].unique()):\n",
    "    if cid == -1:\n",
    "        continue\n",
    "\n",
    "    idxs = df[df[\"cluster_id\"] == cid].index\n",
    "    if len(idxs) == 0:\n",
    "        continue\n",
    "\n",
    "    emb_sub = embeddings[idxs]\n",
    "    centroid = emb_sub.mean(axis=0, keepdims=True)\n",
    "    sims = cosine_similarity(emb_sub, centroid).flatten()\n",
    "    rep_idx = idxs[sims.argmax()]\n",
    "\n",
    "    cluster_reps.append(df.loc[rep_idx, [\"cluster_id\", \"query_id\", \"title\", \"clean_abs\"]])\n",
    "\n",
    "pd.DataFrame(cluster_reps).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12381380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query distribution per cluster\n",
    "cluster_query_summary = (\n",
    "    df[df[\"cluster_id\"] != -1]\n",
    "    .groupby([\"cluster_id\", \"query_id\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    "    .sort_values([\"cluster_id\", \"count\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "cluster_query_summary.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940217ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword extraction per cluster (very simple version)\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_keywords(texts, top_k=10):\n",
    "    tokens = []\n",
    "    for t in texts:\n",
    "        ts = re.findall(r\"[a-zA-Z]+\", t.lower())\n",
    "        tokens.extend(ts)\n",
    "    return Counter(tokens).most_common(top_k)\n",
    "\n",
    "cluster_keywords = {}\n",
    "\n",
    "for cid in sorted(df[\"cluster_id\"].unique()):\n",
    "    cluster_texts = df[df[\"cluster_id\"] == cid][\"clean_abs\"].tolist()\n",
    "    top_words = get_keywords(cluster_texts, top_k=12)\n",
    "    cluster_keywords[cid] = top_words\n",
    "\n",
    "cluster_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d29c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-catalogue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
